---
title: "Problem Section 4"
subtitle:  "Estimation"
graphics: yes
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

* * * 
### Instructions

Please answer the following questions in the order in which they are posed. Add a few empty lines below each and write your answers there. Focus on answering in complete sentences. You will also need scratch paper/pen to work out the answers before typing it.

For help with formatting documents in RMarkdown, please consult [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/). Another option is to search using Google. 

* * *

### Exercises 

1. Suppose $X_1, X_2, \dots, X_n$ are independent random variables with PMF
\begin{eqnarray*}
f(x) &=& \left\{ \begin{array}{cc}
             \frac{1}{3} - \theta_0 & x = 0  \\
             \frac{1}{3} & x =1 \\
             \frac{1}{3} + \theta_0 & x =2 \end{array} \right.
\end{eqnarray*}
where you can assume $-\frac{1}{3} \leq \theta_0 \leq \frac{1}{3}$ so all probabilities are non-zero. 

a. Give an expression for $\hat{\theta}_0^{mom}$, the method of moments estimator of $\theta_0$. 

b. Is your estimator from part a. an unbiased estimator for $\theta_0$?


c. Find the variance of your estimator from part a. Is it a consistent estimator for $\theta_0$? (Hint:  you will need to find $Var\left[X \right]$ to answer this. ) 

d. Based on the sample 0, 0, 1, 0, 1, 2, 1, 0, 0, calculate the value of $\hat{\theta}_0^{mom}$ for this sample.

2. Suppose $x_1 = 8.3, x_2 = 4.9, x_3 = 2.6, x_4 = 6.5$ are drawn independently from a two parameter uniform distribution:
$$f(x) = \frac{1}{2 \theta_2} \ \ \ \theta_1 - \theta_2 < x < \theta_1 + \theta_2.$$

Use the method of moments to find $\hat{\theta}_1^{mom}$ and $\hat{\theta}_2^{mom}$.


3. When sampling from a $Unif(0, \theta_0)$ distribution, we saw that the $X_{max}$ was a biased estimator of $\theta_0$ because
$$E\left[X_{max} \right] = \frac{n}{n+1} \theta_0.$$

a. Create an unbiased estimator based on $X_{max}$. Let's call this estimator $\hat{\theta}_0$. (Hint: an estimator can be based on random variables and numbers like $n$. The only thing off limits are unknown parameters)

b. Derive the variance of $\hat{\theta}_0$. Is its variance larger or smaller than the $Var\left[ X_{max} \right]$? 

c. A common way to compare estimators is via the Mean Square Error (MSE) which is defined as
$$MSE(\hat{\theta}_0) = (bias)^2 + Var\left[\hat{\theta}_0 \right]$$
where the bias term is defined as
$$bias = E\left[\hat{\theta}_0 \right] - \theta_0.$$
The MSE measures how close the estimator falls on the average to the true $\theta_0$. 
For unbiased estimators like $\hat{\theta}_0$, the MSE is simply the variance. Find the MSE of $X_{max}$.

d. Show that despite being a biased estimator, $X_{max}$ has a smaller MSE compared to $\hat{\theta}_0$ uniformly for all possible values of $\theta_0$. In other words show that 
$$MSE(X_{max}) < MSE(\hat{\theta}_0)$$

\emph{This part shows that to achieve a low MSE, we can sacrifice having some bias if the estimator has a relatively small variance. The additional penalty to the variance simply to get unbiasedness is not worth it. This is an example of the bias variance tradeoff.}

