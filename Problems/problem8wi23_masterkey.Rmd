---
title: "Problem Section 8 KEY"
subtitle:  "Empirical P-value"
graphics: yes
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=F,message=F)
library(tidyverse)
library(fastR2)
library(gridExtra)
```

* * * 
### Instructions

Please answer the following questions in the order in which they are posed. Add a few empty lines below each and write your answers there. Focus on answering in complete sentences. You will also need scratch paper/pen to work out the answers before typing it.

For help with formatting documents in RMarkdown, please consult [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/). Another option is to search using Google. 

* * *

### Exercises 

1. Suppose the following sample - denoted by $X_1,X_2,\dots,X_n$ - are drawn from a $Norm(0, \sigma^2_0)$ distribution:
```{r}
x<- c(-0.58319935, -1.36090219,  0.38663763, -1.54365592,  0.87083945, -0.69187830, 0.45898841, -2.82556635,  0.01777137, -0.62753863 , 0.54611381, -1.39731591, -1.72584231,  0.91371529,  0.18096064, -0.53063107, -0.76604739, -1.97107704, 0.56394712,  1.13707563)
```

Suppose we want to test $H_0: \sigma^2_0 = 1$ versus $\sigma^2_0 > 1$ using the sample variance:
$$S^2 = \frac{1}{n-1} \sum\limits_{i=1}^{n} (X_i - \bar{X})^2.$$

a. Calculate the observed value for $S^2$. What direction is predicted for our statistic under $H_1$?

```{r}
obs_s2 <- var(x)

```

The observed value of $S^2$ is `r round(obs_s2, 4)`. I would expect large values for $S^2$ under $H_1$. 

b. Simulate the null sampling distribution of $S^2$. Make a histogram, label it properly and mark the observed value with a vertical line. Calculate the empirical P-value.  (Hint: best to use a looping function such as `lapply`)

```{r label="gen-null-dist", message=F, warning=F}
set.seed(2626)
B <- 10000
sampsize <- length(x)
sigma2_null <- 1

null_sim_df   <- lapply( X = 1:B, FUN = function(i){
                          sim_x = rnorm(n = sampsize, mean = 0, sd = sigma2_null)
                          data.frame( sim_s2 = var(sim_x), 
                                      sim_r = diff( range(sim_x) ) ) })


s2_null_dist <- do.call( rbind, null_sim_df) 

ggplot(data = s2_null_dist,
       mapping = aes( x =  sim_s2) ) +
      geom_histogram(alpha = 0.5) +
  geom_vline( xintercept = obs_s2, color = "red") +
  labs( x = "Values of S2",
        title = "Null sampling distribution of S2",
        subtitle = "observed value as vertical line in red")


## calculate empirical P-value

empirical_p_s2 <- s2_null_dist %>% 
                   summarise( pvalue_s2 =  sum( sim_s2 >= obs_s2)/B   )
empirical_p_s2

```

c. Suppose we instead decide to the use the Range $R = X_{max} - X_{min}$ as our test statistic. Repeat parts a - c using $R$. (Write your code for c flexibly so you can calculate multiple statistics from the simulated data.)

```{r}
#calculate observed value for range
obs_r <- diff(range(x))

#plotting null dist of range
ggplot(data = s2_null_dist,
       mapping = aes( x =  sim_r) ) +
      geom_histogram(alpha = 0.5) +
  geom_vline( xintercept = obs_r, color = "red") +
  labs( x = "Values of R",
        title = "Null sampling distribution of R",
        subtitle = "observed value as vertical line in red")


## calculate empirical P-value

empirical_p_r <- s2_null_dist %>% 
                   summarise( pvalue_r =  sum( sim_r >= obs_r)/B   )
empirical_p_r
```


d. Suppose we decide to use a level $\alpha=0.05$ test. What values of $S^2$ will you reject $H_0$ for? Same question for $R$. (Hint: find the 0.95 quantile of the null sampling distribution of each statistic. Think about why.) 

In both cases larger statistics are more in favor of the alternative hypothesis. We would reject the null if less than 5% of our values in the null sampling distribution are greater than our observed value. This thus implies that if our observed statistic is greater than the 95'th percentile, than we would reject the null at a $\alpha = .05$ significance level. 

We have the 95th percentiles for the two null sampling distributions as:

```{r}
critical_values <- s2_null_dist %>% 
                    summarise(s2crit = quantile(sim_s2, 0.95),
                              rcrit = quantile(sim_r,0.95))
critical_values
```


Thus if our sampling statistic in either test is greater than the respective value above, we would reject the null hypothesis at a 5% level. 

e. For the level $\alpha = 0.05$ test, compare the Type II error probabilities for $S^2$ and $R$ when $\sigma^2_0 = 3$. Remember, the Type II error rate is the probability that we fail to reject $H_0$ when $H_1$ is true. How do they do?   (You will need to simulate data under the alternative to calculate the Type II error rate)

```{r label="sim-alt-dist"}
set.seed(151)
sigma2_alt <- sqrt(3)
sim_df_alt <- lapply(X=1:B, FUN=function(X){
                           sampdata = rnorm(n=20,sd=sigma2_alt)
                           data.frame(s2 = var(sampdata), 
                                      r = diff(range(sampdata)))
})

sim_stats_alt <- do.call(rbind, sim_df_alt)

sim_stats_alt %>% summarise( mean(s2 < critical_values$s2crit),
                             mean(r < critical_values$rcrit))

p1 <- ggplot(data=NULL,aes(x=sim_stats_alt$s2,y=..density..))+
  geom_histogram(bins = 1 + 1.322*log(B))+
  labs(title = "Simulated Alternative Sampling Distribution of S^2",
       x = "Sample S^2 Values")+
  geom_vline(xintercept = critical_values$s2crit,col="red")


p2 <- ggplot(data=NULL,aes(x=sim_stats_alt$r,y=..density..))+
  geom_histogram(bins = 1 + 1.322*log(B))+
  labs(title = "Simulated Alternative Sampling Distribution of range",
       x = "Sample range Values")+
  geom_vline(xintercept = critical_values$rcrit,col="red")
  
grid.arrange(p1,p2)

```
f. Think of a different statistic - call it $T$ - that will be sensitive to large values of $\sigma_0$. Repeat a - e using $T$. 

Let $T$ be the the value with the largest absolute value of our data, that is the point that is furthest away from the center of the graph (negative or positive). We expect as the standard deviation, $\sigma$ grows, this value $T$ will also grow. 

We see that in our data set we have an observed $T$ of:

```{r}
obs_T <- max(abs(x))
obs_T
```

We can create a null sampling distribution as we did before:

```{r}
set.seed(2626)
B <- 10000

null_sim_df <- lapply(X=1:B, FUN=function(X){
                           sampdata = rnorm(n=20,sd=1)
                           data.frame(t = max(abs(sampdata)))
})

null_sim_stats <- do.call(rbind, null_sim_df)


ggplot(data=NULL,aes(x=null_sim_stats$t,y=..density..))+
  geom_histogram(bins = 1 + 1.322*log(B))+
  labs(title = "Simulated Null Sampling Distribution of Absolute Max",
       x = "Sample Abs-Max Values")+
  geom_vline(xintercept = obs_T,col="red")
```

We see here that we would have a p-val of:

```{r}
null_sim_stats %>% summarize(p_val = mean(t>=obs_T))
```

We also see we have our critical value as:

```{r}
critical_values <- null_sim_stats %>% 
                    summarise(t_crit = quantile(t, 0.95))
critical_values
```

We may now find the Type 2 Error rate when $\sigma^2_0 = 3$ 

```{r}
sim_df_alt <- lapply(X=1:B, FUN=function(X){
                           sampdata = rnorm(n=20,sd=sqrt(3))
                           data.frame(t = abs(max(sampdata)))
})

sim_stats_alt <- do.call(rbind, sim_df_alt)


ggplot(data=NULL,aes(x=sim_stats_alt$t,y=..density..))+
  geom_histogram(bins = 1 + 1.322*log(B))+
  labs(title = "Simulated Alternative Sampling Distribution of T (Abs-Max)",
       x = "Sample T Values")+
  geom_vline(xintercept = critical_values$t_crit,col="red")
```

We thus know the proportion of values below our critical T value in the above distribution will be our Type 2 error rate. We have this as:

```{r}
sim_stats_alt %>% summarise(type_2_error =  mean(t < critical_values$t_crit))
```


